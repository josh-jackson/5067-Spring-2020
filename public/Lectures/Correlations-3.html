<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Correlations</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josh Jackson" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 50% !important;
}
&lt;/style&gt;








# Last time/this time

- Looked at glm models and showed how they are the same as t-tests and ANOVA
- Moving toward continuous predictor models

---
# Relationships

- What is the relationship between IV and DV?

- Measuring relationships depend on type of measurement

- You have primarily been working wtih categorical IVs (t-test, ANOVA, chi-square)

---
# Scatter Plot with best fit line


&lt;img src="Correlations-3_files/figure-html/unnamed-chunk-2-1.png" width="504" /&gt;

---
# Review of Dispersion

Variation (sum of squares)
`$$SS = {\sum{(x-\bar{x})^2}}$$`
`$$SS = {\sum{(x-\mu)^2}}$$`
---
# Review of Dispersion
Variance
`$$s^{2} = {\frac{\sum{(x-\bar{x})^2}}{N-1}}$$`
`$$\sigma^{2} = {\frac{\sum{(x-\mu)^2}}{N}}$$`

---
# Review of Dispersion
Standard Deviation
`$$s = \sqrt{\frac{\sum{(x-\bar{x})^2}}{N-1}}$$`
`$$\sigma = \sqrt{\frac{\sum{(x-\mu)^2}}{N}}$$`

---

# Associations

- ie relationships
- to look at continuous variable associations we need to think in terms of how variables relate to one another

---
# Associations
Covariation (cross products)
`$$SS = {\sum{(x-\bar{x})(y-\bar{y})}}$$`




`$$SS = {\sum{{(x-\mu_{x}})(y-\mu_{y})}}$$`


---
# Associations
Covariance
`$$cov_{xy}^{2} = {\frac{\sum{(x-\bar{x})(y-\bar{y})}}{N-1}}$$`

`$$\sigma_{xy}^{2} = {\frac{\sum{(x-\mu_{x})(y-\mu_{y})}}{N}}$$`

--
- Covariance matrix is basis for many analyses  
- What are some issues that may arise when comparing covariances? 

---
# Associations
Correlations
`$$r_{xy} = {\frac{\sum({z_{x}z_{y})}}{N}}$$`

`$$\rho_{xy} = {\frac{cov(X,Y)}{\sigma_{x}\sigma_{y}}}$$`


Many other formulas exist for specific types of variables (eg dichotomous). These were more helpful when we computed everything by hand (more on this later)


---
# Associations
Correlations

- How much two variables are linearly related
- -1 to 1 
- Invariant to changes in mean or scaling
- Most common (and basic) effect size measure
- Will use to build our regression model

---

.pull-left[
&lt;img src="Correlations-3_files/figure-html/unnamed-chunk-3-1.png" width="504" /&gt;
]

.pull-right[
- This will become our regression line. Right now it is our correlation line. They are the same! 
- The correlation summarizes the association between two continuous variables. So does our regression (and more!)
- We will use this build up our regression model, but in the mean time you can do a lot of things with just correlations. 
]

---

# Correlations 
Hypothesis testing

`$$H_{0}: \rho_{xy} = 0$$`
`$$H_{A}: \rho_{xy} \neq 0$$`

Assumes: 
- Observations are independent
- Symmetric bivariate distribution (joint probability distribution)


---

&lt;img src="Correlations-3_files/figure-html/unnamed-chunk-4-1.png" width="504" /&gt;

---
# Correlations 


`$$H_{0}: \rho_{xy} = 0$$`

`$$H_{A}: \rho_{xy} \neq 0$$`

---
# Test statistic

As will most all test statistics they take hte form of "signal" divided by "noise"
`$$t = {\frac{r}{SE_{r}}}$$`

Our standard error is interpreted like other standard errors 
`$$t = {\frac{r}{\sqrt{\frac{1-r^{2}}{N-2}}}}$$`

df = N-2

---

# Effect size 

- The strength of relationship between two variables

- Ω2, η2, cohen’s d, cohen’s f, hedges g, R2 , Risk-ratio, etc

- Significance is a function of effect size and sample size

- Statistical significance ≠ practical significance

---
# Effect size  
How big is practical?

- Cohen (.1, .3., .5)
- Meyer &amp; Hemphill .3 is average 
- Rosenthaul:   

Drug TX?  | Alive |  Dead
----------|-------|--------
Treatment |  65   |  35
No Tx     |  35   |  65


---
# What is the size of the correlation?
- Chemotherapy and breast cancer survival?   
- Batting ability and hit success on a single at bat?   
- Antihistamine use and reduced sneezing/runny nose?   
- Combat exposure and PTSD?   
- Ibuprofen on pain reduction?   
- Gender and weight?   
- Therapy and well being?   
- Observer ratings of attractiveness?   
- Gender and arm strength?   
- Nearness to equator and daily temperature for U.S.?   

---
# What is the size of the correlation?
- Chemotherapy and breast cancer survival? (.03)   
- Batting ability and hit success on a single at bat? (.06)   
- Antihistamine use and reduced sneezing/runny nose? (.11)   
- Combat exposure and PTSD? (.11)   
- Ibuprofen on pain reduction? (.14)   
- Gender and weight? (.26)   
- Therapy and well being? (.32)   
- Observer ratings of attractiveness? (.39)   
- Gender and arm strength? (.55)   
- Nearness to equator and daily temperature for U.S.? (.60)

---

# Visualizing correlations

https://rpsychologist.com/d3/correlation/

http://guessthecorrelation.com

---
# Questions to ask yourself:
What is your N?  
What is the typical effect size in the field?  
Study design?  
What is your DV?  
Importance (reaction time vs cancer)?  
Same method as IV (method variance)?  

---
# Power calculations

.small[

```r
library(pwr)
pwr.r.test(n = , r = .1, sig.level = .05 , power = .8)
```

```
## 
##      approximate correlation power calculation (arctangh transformation) 
## 
##               n = 781.7516
##               r = 0.1
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
```

```r
pwr.r.test(n = , r = .3, sig.level = .05 , power = .8)
```

```
## 
##      approximate correlation power calculation (arctangh transformation) 
## 
##               n = 84.07364
##               r = 0.3
##       sig.level = 0.05
##           power = 0.8
##     alternative = two.sided
```
]

---
# Power calculations
- But what is your confidence? 
- N = 84 gives you CI[.09, .48]
- Schönbrodt &amp; Perugini (2013) suggest correlations 'stabilize' at 250+ regardless of effect size
- CI[.19, .39]

---
# Fisher’s r to z’ transformation
- If we want to make calculations based on `\(\rho \neq 0\)` then we will run into a skewed sampling distribution
 
&lt;img src="Correlations-3_files/figure-html/unnamed-chunk-6-1.png" width="504" /&gt;

---
# Fisher’s r to z’ transformation
- Skewed sampling distribution will rear its head when: 
    * `\(H_{0}: \rho \neq 0\)`
    * Calculating confidence intervals
    * Testing two correlations against one another
- r to z': 

`$$z^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}$$`
---
# Fisher’s r to z’ transformation

.center[
![](../img/fisher.png)



---
#  Steps for computing confidence interval

1. Transform r into z'  
2. Compute CI as you normally would using z'  
3. revert back to r  

$$ r = {\frac{e^{2z'}-1}{e^{2z'}+1}} $$

---
# How to do in R

```r
library(psych)
fisherz(r)
fisherz2r(z)
```

---
# Two independent groups test 
-Does the correlation in group 1 differ from the correlation in group 2? 
`$$H_{A}: \rho_{1} = \rho_{2}$$`
`$$H_{A}: \rho_{1} \neq \rho_{2}$$`
-Normally distributed
`$$Z = {\frac{z'_{1}-z'_{2}}{se_{z1-z2}}}$$`

---

# Two independent groups test
- different standard error compared to 1 sample test
`$$Z = {\frac{z'_{1}-z'_{2}}{se_{z1-z2}}}$$`
`$$se_{z1-z2} = {\sqrt {se_{z1} + se_{z1}}} = {\sqrt {\frac{1}{n_{1}-3}+{\frac{1}{n_{2}-3}}}}$$`
- but probably best to do this test in another framework (e.g., GLM via interaction, SEM)

---

# Other correlation tests:
1. Set of correltions
2. Dependent correlations (i.e., within same group)  
  These are more easily tested via Structural Equation Modeling (SEM)
3. Intra Class Correlation (ICC)

- Again, best to do these tests in another framework (e.g., GLM, SEM, MLM)
  
---
# Factors that influence r (and most other test statistics)
1. Restriction of range (GRE scores and success)
2. Very skewed distributions (smoking and health)
3. Non-linear associations
4. Measurement overlap (modality and content)
5. Reliability

---
# Reliability
- All measurement includes error
- Score = true score + measurement error (CTT version)
- Reliability assesses the consistency of measurement

---
# Reliability
- All measurement includes error
- Score = true score + measurement error (CTT version)
- Reliability assesses the consistency of measurement
  
  Which would you rather have?  
  - 1 item final exam versus 30 item?   
  - assessment via trained clinician vs tarot cards?  
  - fMRI during minor earthquake vs no earthquake?  

---  
# Reliability

- Cannot correlate error (randomness) with something
- Because we do not measure our variables perfectly we get lower correlations compared to true correlations
- If we want to have a valid measure it better be a reliable measure

---
# Reliability

- think of reliability as a correlation with a measure and itself in a different world, at a different time, or a different but equal version

`$$r_{XX}$$`

---
# Reliability

- true score variance divided by observed variance
- how do you assess theoretical variance i.e., true score variance? 

`$$r_{XY} = r_{X_{T} Y_{T}} {\sqrt{r_{XX}r_{YY}}}$$`
`$$r_{XY} = .6 {\sqrt {(.70) (.70)}}$$`

---
# Reliability

`$$r_{X_{T} Y_{T}} =  {\frac {r_{XY}} {\sqrt{r_{XX}r_{YY}}}}$$`


`$$r_{X_{T} Y_{T}} =   {\frac {.30} {\sqrt{(.70)(.70)}}}$$`

---
# Most common ways to assess

- cronbachs alpha 

```r
library (psych)
alpha(measure)
## Gives average split half correlation
## Can tell you if you are assessing a single construct
```
- test - retest reliability
- Kappa or ICC

---
# Reliability

- if you are going to measure something do it well
- applies to ALL IVs and DVs, and all designs
- remember this when interpretting other's research

---
# Types of correlations
- Many ways to get at relationship between two variables
- Statistically the different types are almost exactly the same
- Exist for historical reasons

---
# Types of correlations
1. Point Biserial
    +  continuous and dichtomous
2. Phi coefficient
    + both dichotomous
3. Spearman rank order
    + ranked data (nonparametric)
4. Biserial (assumes dichotomous is continous)
5. Tetrachoric (assumes dichotomous is continous)
    + both dichotomous
6. Polychoric (assumes continous)
    + ordinal
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
