---
title: "Correlations"
author: Josh Jackson
date: "1-21-20"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    lib_dir: libs
    css: xaringan-themer.css
    seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
---

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_light(
  base_color = "#23395b",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Droid Mono"),
)
```

<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 50% !important;
}
</style>



```{r setup,echo=FALSE}
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)

options(scipen=999)
```




# Last time/this time

- Looked at glm models and showed how they are the same as t-tests and ANOVA
- Moving toward continuous predictor models

---
# Relationships

- What is the relationship between IV and DV?

- Measuring relationships depend on type of measurement

- You have primarily been working with categorical IVs (t-test, ANOVA, chi-square)

---
# Scatter Plot with best fit line
```{r,echo=FALSE, message=FALSE, cache=TRUE}
library(psychTools)
library(ggplot2)
data(galton)
```

```{r, echo=FALSE, cache=TRUE}
ggplot(galton, aes(x=parent, y=child)) +
    geom_point() +    
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE) +     # Don't add shaded confidence region
    labs(x = "parent height", y = "child height")
```

---
# Review of Dispersion

Variation (sum of squares)
$$SS = {\sum{(x-\bar{x})^2}}$$
$$SS = {\sum{(x-\mu)^2}}$$
---
# Review of Dispersion
Variance
$$s^{2} = {\frac{\sum{(x-\bar{x})^2}}{N-1}}$$
$$\sigma^{2} = {\frac{\sum{(x-\mu)^2}}{N}}$$

---
# Review of Dispersion
Standard Deviation
$$s = \sqrt{\frac{\sum{(x-\bar{x})^2}}{N-1}}$$
$$\sigma = \sqrt{\frac{\sum{(x-\mu)^2}}{N}}$$

---

# Associations

- ie relationships
- to look at continuous variable associations we need to think in terms of how variables relate to one another

---
# Associations
Covariation (cross products)
$$SS = {\sum{(x-\bar{x})(y-\bar{y})}}$$




$$SS = {\sum{{(x-\mu_{x}})(y-\mu_{y})}}$$


---
# Associations
Covariance
$$cov_{xy}^{2} = {\frac{\sum{(x-\bar{x})(y-\bar{y})}}{N-1}}$$

$$\sigma_{xy}^{2} = {\frac{\sum{(x-\mu_{x})(y-\mu_{y})}}{N}}$$

--
- Covariance matrix is basis for many analyses  
- What are some issues that may arise when comparing covariances? 

---
# Associations
Correlations
$$r_{xy} = {\frac{\sum({z_{x}z_{y})}}{N}}$$

$$\rho_{xy} = {\frac{cov(X,Y)}{\sigma_{x}\sigma_{y}}}$$


Many other formulas exist for specific types of variables (eg dichotomous). These were more helpful when we computed everything by hand (more on this later)


---
# Associations
Correlations

- How much two variables are linearly related
- -1 to 1 
- Invariant to changes in mean or scaling
- Most common (and basic) effect size measure
- Will use to build our regression model

---

.pull-left[
```{r, echo=FALSE, cache=TRUE}
ggplot(galton, aes(x=parent, y=child)) +
    geom_point() +    
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE) +     # Don't add shaded confidence region
    labs(x = "parent height", y = "child height")
```
]

.pull-right[
- This will become our regression line. Right now it is our correlation line. They are the same! 
- The correlation summarizes the association between two continuous variables. So does our regression (and more!)
- We will use this build up our regression model, but in the mean time you can do a lot of things with just correlations. 
]

---

# Correlations 
Hypothesis testing

$$H_{0}: \rho_{xy} = 0$$
$$H_{A}: \rho_{xy} \neq 0$$

Assumes: 
- Observations are independent
- Symmetric bivariate distribution (joint probability distribution)


---

```{r, echo=FALSE,cache=TRUE}
mu1<-0 # setting the expected value of x1
mu2<-0 # setting the expected value of x2
s11<-10 # setting the variance of x1
s12<-15 # setting the covariance between x1 and x2 
s22<-10 # setting the variance of x2
rho<-0.5 # setting the correlation coefficient between x1 and x2 
x1<-seq(-10,10,length=41) # generating the vector series x1 
x2<-x1 # copying x1 to x2
f<-function(x1,x2)
{
term1<-1/(2*pi*sqrt(s11*s22*(1-rho^2))) 
term2<--1/(2*(1-rho^2))
term3<-(x1-mu1)^2/s11
term4<-(x2-mu2)^2/s22 
term5<--2*rho*((x1-mu1)*(x2-mu2))/(sqrt(s11)*sqrt(s22))
term1*exp(term2*(term3+term4-term5))
} # setting up the function of the multivariate normal density
#
z<-outer(x1,x2,f) # calculating the density values
#
persp(x1, x2, z,
main="Joint Probability Distribution", sub=expression(italic(f)~(bold(x))==frac(1,2~pi~sqrt(sigma[11]~ sigma[22]~(1-rho^2)))~phantom(0)^bold(.)~exp~bgroup("{", list(-frac(1,2(1-rho^2)),
bgroup("[", frac((x[1]~-~mu[1])^2, sigma[11])~-~2~rho~frac(x[1]~-~mu[1], sqrt(sigma[11]))~ frac(x[2]~-~mu[2],sqrt(sigma[22]))~+~ frac((x[2]~-~mu[2])^2, sigma[22]),"]")),"}")),
col="lightgreen",
theta=30, phi=20,
r=50,
d=0.1,
expand=0.5,
ltheta=90, lphi=180,
shade=0.75,
ticktype="detailed",
nticks=5) 

# produces the 3-D plot
mtext(expression(list(mu[1]==0,mu[2]==0,sigma[11]==10,sigma[22]==10,sigma[12 ]==15,rho==0.5)), side=3) 
# adding a text line to the graph


```

---
# Correlations 


$$H_{0}: \rho_{xy} = 0$$

$$H_{A}: \rho_{xy} \neq 0$$

---
# Test statistic

As with most all test statistics they take the form of "signal" divided by "noise"
$$t = {\frac{r}{SE_{r}}}$$

Our standard error is interpreted like other standard errors 
$$t = {\frac{r}{\sqrt{\frac{1-r^{2}}{N-2}}}}$$

df = N-2

---

# Effect size 

- The strength of relationship between two variables

- Ω2, η2, cohen’s d, cohen’s f, hedges g, R2 , Risk-ratio, etc

- Significance is a function of effect size and sample size

- Statistical significance ≠ practical significance

---
# Effect size  
How big is practical?

- Cohen (.1, .3., .5)
- Meyer & Hemphill .3 is average 
- Rosenthaul:   

Drug TX?  | Alive |  Dead
----------|-------|--------
Treatment |  65   |  35
No Tx     |  35   |  65


---
# What is the size of the correlation?
- Chemotherapy and breast cancer survival?   
- Batting ability and hit success on a single at bat?   
- Antihistamine use and reduced sneezing/runny nose?   
- Combat exposure and PTSD?   
- Ibuprofen on pain reduction?   
- Gender and weight?   
- Therapy and well being?   
- Observer ratings of attractiveness?   
- Gender and arm strength?   
- Nearness to equator and daily temperature for U.S.?   

---
# What is the size of the correlation?
- Chemotherapy and breast cancer survival? (.03)   
- Batting ability and hit success on a single at bat? (.06)   
- Antihistamine use and reduced sneezing/runny nose? (.11)   
- Combat exposure and PTSD? (.11)   
- Ibuprofen on pain reduction? (.14)   
- Gender and weight? (.26)   
- Therapy and well being? (.32)   
- Observer ratings of attractiveness? (.39)   
- Gender and arm strength? (.55)   
- Nearness to equator and daily temperature for U.S.? (.60)

---

# Visualizing correlations

https://rpsychologist.com/d3/correlation/

http://guessthecorrelation.com

---
# Questions to ask yourself:
What is your N?  
What is the typical effect size in the field?  
Study design?  
What is your DV?  
Importance (reaction time vs cancer)?  
Same method as IV (method variance)?  

---
# Power calculations

.tiny[
```{r}
library(pwr)
pwr.r.test(n = , r = .1, sig.level = .05 , power = .8)
pwr.r.test(n = , r = .3, sig.level = .05 , power = .8)
```
]

---
# Power calculations
- But what is your confidence? 
- N = 84 gives you CI[.09, .48]
- Schönbrodt & Perugini (2013) suggest correlations 'stabilize' at 250+ regardless of effect size
- CI[.19, .39]

---
# Fisher’s r to z’ transformation

.pull-left[
- If we want to make calculations based on $\rho \neq 0$ then we will run into a skewed sampling distribution
]



.pull-right[
 
```{r,echo=FALSE, cache=TRUE }
library(ggplot2)

r=0.95
n=50
rep=10000
z=.5*log((1+r)/(1-r))
se=1/sqrt(n-3)
zvec=rnorm(rep)*se+z
ivec=(exp(2*zvec)-1)/(exp(2*zvec)+1)
correlation <- as.data.frame(ivec)
correlation$cor <- correlation$ivec
ggplot(correlation, aes(cor)) + geom_histogram(breaks=seq(.85, .99, by = .001), aes(y = ..density..)) + geom_density()
```

]


---
# Fisher’s r to z’ transformation
- Skewed sampling distribution will rear its head when: 
    * $H_{0}: \rho \neq 0$
    * Calculating confidence intervals
    * Testing two correlations against one another
- r to z': 

$$z^{'} = {\frac{1}{2}}ln{\frac{1+r}{1-r}}$$
---
# Fisher’s r to z’ transformation

```{r out.width = '50%', echo = FALSE}

knitr::include_graphics("../img/fisher.png")
```

---
# Fisher’s r to z’ transformation

- Because the distribution is normal, you can use the z' as you would with the z tests last semester. 1 sample, 2 sample, calculate confidence interval...

- For a one sample test the standard error is 
$$se_{z'} = {\frac{1}{\sqrt {N-3}}}$$ 

---
#  Steps for computing confidence interval

1. Transform r into z'  
2. Compute CI as you normally would using z'  
3. Revert back to r  

$$ r = {\frac{e^{2z'}-1}{e^{2z'}+1}} $$

---
# How to do in R
```{r, eval=FALSE}
library(psych)
fisherz(r)
fisherz2r(z)
```

---
# Two independent groups test 
-Does the correlation in group 1 differ from the correlation in group 2? 
$$H_{A}: \rho_{1} = \rho_{2}$$
$$H_{A}: \rho_{1} \neq \rho_{2}$$
-Normally distributed
$$Z = {\frac{z'_{1}-z'_{2}}{se_{z1-z2}}}$$

---

# Two independent groups test
- Different standard error compared to 1 sample test
$$Z = {\frac{z'_{1}-z'_{2}}{se_{z1-z2}}}$$
$$se_{z1-z2} =  {\sqrt {\frac{1}{n_{1}-3}+{\frac{1}{n_{2}-3}}}}$$
- But probably best to do this test in another framework (e.g., GLM via interaction, SEM)

---

# Other correlation tests:
1. Set of correlations
2. Dependent correlations (i.e., within same group)  
  These are more easily tested via Structural Equation Modeling (SEM)
3. Intra Class Correlation (ICC)

- Again, best to do these tests in another framework (e.g., GLM, SEM, MLM)
  
---
# Factors that influence r (and most other test statistics)
1. Restriction of range (GRE scores and success)
2. Very skewed distributions (smoking and health)
3. Non-linear associations
4. Measurement overlap (modality and content)
5. Reliability

---
# Reliability
- All measurement includes error
- Score = true score + measurement error (CTT version)
- Reliability assesses the consistency of measurement

---
# Reliability
- All measurement includes error
- Score = true score + measurement error (CTT version)
- Reliability assesses the consistency of measurement
  
  Which would you rather have?  
  - 1 item final exam versus 30 item?   
  - assessment via trained clinician vs tarot cards?  
  - fMRI during minor earthquake vs no earthquake?  

---
# Reliability

- Cannot correlate error (randomness) with something
- Because we do not measure our variables perfectly we get lower correlations compared to true correlations
- If we want to have a valid measure it better be a reliable measure

---
# Reliability

- Think of reliability as a correlation with a measure and itself in a different world, at a different time, or a different but equal version

$$r_{XX}$$

---
# Reliability

- True score variance divided by observed variance
- How do you assess theoretical variance i.e., true score variance? 

$$r_{XY} = r_{X_{T} Y_{T}} {\sqrt{r_{XX}r_{YY}}}$$
$$r_{XY} = .6 {\sqrt {(.70) (.70)}}$$

---
# Reliability

$$r_{X_{T} Y_{T}} =  {\frac {r_{XY}} {\sqrt{r_{XX}r_{YY}}}}$$


$$r_{X_{T} Y_{T}} =   {\frac {.30} {\sqrt{(.70)(.70)}}}$$

---
# Most common ways to assess

- Cronbachs alpha 
```{r, eval=FALSE}
library (psych)
alpha(measure)
## Gives average split half correlation
## Can tell you if you are assessing a single construct
```
- Test - retest reliability
- Kappa or ICC

---
# Reliability

- if you are going to measure something do it well
- applies to ALL IVs and DVs, and all designs
- remember this when interpreting others research

---
# Types of correlations
- Many ways to get at relationship between two variables
- Statistically the different types are almost exactly the same
- Exist for historical reasons

---
# Types of correlations
1. Point Biserial
    +  continuous and dichotomous
2. Phi coefficient
    + both dichotomous
3. Spearman rank order
    + ranked data (nonparametric)
4. Biserial (assumes dichotomous is continuous)
5. Tetrachoric (assumes dichotomous is continuous)
    + both dichotomous
6. Polychoric (assumes continuous)
    + ordinal
  



