<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Univariate Regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="Josh Jackson" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 65% !important;
}
&lt;/style&gt;

# Last time/this time




- Last time, we looked at associations with two continuous variables as indexed via correlations
- This time, we will extend regression framework to incorporate continous predictors.

---
# We want to make eduated guesses
- E(Y|X)
- The regression of Y (DV) on X (IV) corresponds to the line that gives the mean value of Y corresponding to each possible value of X
- "Our best guess" regardless of whether our model includes categories or continuous predictor variables
- We will evaluate our guesses based on how far we are away from the mean...but how do we come up with those guesses in the first place?


---
# Regression Equation

`$$Y = b_{0} + b_{1}X +e$$`
`$$\hat{Y} = b_{0} + b_{1}X$$`



---
# OLS
- How do we find the regression estimates and thus our predicted values? 
- Ordinary Least Squares (OLS) estimation 
- Minimizes deviations 

`$$min\sum(Y_{i}-\hat{Y})^{2}$$` 

- Other estimation procedures possible (and necessary in some cases)

---



&lt;img src="regression-4_files/figure-html/unnamed-chunk-2-1.png" width="504" /&gt;

---

&lt;img src="regression-4_files/figure-html/unnamed-chunk-3-1.png" width="504" /&gt;


---

&lt;img src="regression-4_files/figure-html/unnamed-chunk-4-1.png" width="504" /&gt;



---

&lt;img src="regression-4_files/figure-html/unnamed-chunk-5-1.png" width="504" /&gt;


---
# Regression coefficient
`$$b_{1} = \frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \frac{s_{y}}{s_{x}}$$`
--

- The regression coefficient (slope) equals the estimated change in Y for a 1-unit change in X  
- For nominal variables using dummy coding (0 and 1s) the regression coefficient equals the difference in means between the two groups (remember, correlation can be used for nominal variables) 

---
# Standardized regression
- Regression using z-scores for Y and X
- Correlation equals standardized regression coefficent
`$$b_{1} = r_{xy} \frac{s_{y}}{s_{x}}$$`
`$$r_{xy} = b_{1} \frac{s_{x}}{s_{y}}$$`
`$$\beta_{1} = b_{1}^{*} = r_{xy}$$`  

---
# Standardized regression equation
`$$Y = b_{1}^{*}X + e$$`
- What is missing? 
- What does this equation tell you about the function of the intercept? 

---
#  Raw score regression equation
- Re-write equation to include means of Y and X 
- Intercept serves to adjust for differences in means between X and Y

`$$\hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X})$$`
- If standardized, intercept drops out, otherwise intercept is where regression line crosses the y-axis at X = 0  
- Notice that when X = `\(\bar{X}\)` the regression line goes through  `\(\bar{Y}\)`. This is true for all regressions such that the regression line must pass through `\(\bar{X}\)` and `\(\bar{Y}\)`


---
## Example
.tiny[

```
## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67 &lt;0.0000000000000002 ***
## child        0.32565    0.02073   15.71 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 0.00000000000000022
```
]

---

&lt;img src="regression-4_files/figure-html/unnamed-chunk-7-1.png" width="504" /&gt;

---
# ANOVA table

```r
anova(fit.1)
```

```
## Analysis of Variance Table
## 
## Response: parent
##            Df  Sum Sq Mean Sq F value                Pr(&gt;F)    
## child       1  623.26  623.26  246.84 &lt; 0.00000000000000022 ***
## Residuals 926 2338.10    2.52                                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```


---
# Exploring the lm object


```r
library(broom)
galton.data.1 &lt;- augment(fit.1, galton.data)
head(galton.data.1)
```

```
## # A tibble: 6 x 9
##   parent child .fitted .se.fit .resid    .hat .sigma  .cooksd .std.resid
##    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
## 1   70.5  61.7    66.2   0.142  4.27  0.00802   1.58 0.0295        2.70 
## 2   68.5  61.7    66.2   0.142  2.27  0.00802   1.59 0.00833       1.44 
## 3   65.5  61.7    66.2   0.142 -0.728 0.00802   1.59 0.000855     -0.460
## 4   64.5  61.7    66.2   0.142 -1.73  0.00802   1.59 0.00482      -1.09 
## 5   64    61.7    66.2   0.142 -2.23  0.00802   1.59 0.00801      -1.41 
## 6   67.5  62.2    66.4   0.133  1.11  0.00698   1.59 0.00172       0.701
```


---
# Y-hats vs Ys

```r
psych::describe(galton.data.1$.fitted)      
```

```
##    vars   n  mean   sd median trimmed  mad   min   max range  skew kurtosis
## X1    1 928 68.31 0.82  68.34   68.32 0.97 66.23 70.14  3.91 -0.09    -0.35
##      se
## X1 0.03
```


```r
psych::describe(galton.data.1$parent)
```

```
##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05 0.06
```

---

&lt;img src="regression-4_files/figure-html/unnamed-chunk-12-1.png" width="504" /&gt;


---
# Residuals vs Ys

.pull-left[
`$$\epsilon \sim N(0,\sigma)$$` 
- variation that is left over in Y, after accounting for X
]

.pull-right[

```r
psych::describe(galton.data.1$.resid)
```

```
##    vars   n mean   sd median trimmed  mad   min  max range skew kurtosis   se
## X1    1 928    0 1.59  -0.15    0.02 1.52 -4.67 4.27  8.94 -0.1    -0.17 0.05
```

```r
psych::describe(galton.data.1$parent)
```

```
##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05 0.06
```
]

---
&lt;img src="regression-4_files/figure-html/unnamed-chunk-14-1.png" width="504" /&gt;


---
# Residuals

- Dispersion of residuals can be thought of as what is left over in Y that is not explained by our model. As residuals get smaller on average so will the SD of the residuals. 
- Sigma (the SD of residuals) can be thought of how much left over in Y that we cannot explain by our model

---
# Residuals are not assocaited with X



---
## and yhat
.pull-left[

```r
ggplot(galton.data.1, aes(x=.resid, y=.fitted)) +
    geom_point() +    
    geom_smooth(method=lm)
```
]
.pull-right[
&lt;img src="regression-4_files/figure-html/unnamed-chunk-17-1.png" width="504" /&gt;
]

---
# r(fitted and X) = 1
&lt;img src="regression-4_files/figure-html/unnamed-chunk-18-1.png" width="504" /&gt;

---
- Residuals are not correlated with X and `\(\hat{Y}\)` because those two are perfectly correlated with one another.
- X and `\(\hat{Y}\)` represent the *same* information. We use our model (X) to make a prediction ( `\(\hat{Y}\)` ). The predictions are entirely based on the model. 
- There is no correlation between residuals with X and `\(\hat{Y}\)` because they are created by subtracting them out of Y ( `\(\varepsilon\)`  = Y- `\(\hat{Y}\)`)
- Sigma (the SD of residuals) can be thought of how much left over in Y after we take out all the information our model provides

---
# Statistical Inference
- The way the world is = our model + error
- How good is our model? Does it "fit" the data well? 
 
- Consider the case with no correlation btw X and Y
`$$\hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X})$$`
`$$\hat{Y} = \bar{Y}$$`

---
# Partitioning variance in Y
- To the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction

`$$\sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$`

- SS total = SS regression + SS residual (or error)

`$$\frac{s_{regression}^2}{s_{y}^2} = \frac{SS_{regression}}{SS_{Y}} = R^2$$`


---
# Example

```r
summary(fit.1)$r.squared
```

```
## [1] 0.2104629
```

---

```r
cor.test(galton.data$parent, galton.data$child)
```

```
## 
## 	Pearson's product-moment correlation
## 
## data:  galton.data$parent and galton.data$child
## t = 15.711, df = 926, p-value &lt; 0.00000000000000022
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4064067 0.5081153
## sample estimates:
##       cor 
## 0.4587624
```

---
## Calculating R2


```
## Analysis of Variance Table
## 
## Response: parent
##            Df  Sum Sq Mean Sq F value                Pr(&gt;F)    
## child       1  623.26  623.26  246.84 &lt; 0.00000000000000022 ***
## Residuals 926 2338.10    2.52                                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

`$$\frac{SS_{regression}}{SS_{Y}} = R^2$$`

`$${SS_{residual}} = (1- R^2){SS_{Y}}$$`
---
# Mean square error (MSE)
- AKA means square residual/within
- AKA square of residual standard error/deviation (sigma)
- Unbiased estimate of error variance
- Measure of discrepancy between the data and the model
- The MSE is the variance of data around the fitted regression line
- Just like MSwithin was variance around predicted group means

---
## Residual standard error
MSE = 2.52 
.tiny[

```
## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67 &lt;0.0000000000000002 ***
## child        0.32565    0.02073   15.71 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 0.00000000000000022
```
]

---
# Residual standard error and sigma
- So many names to represent the spread of data around the regression line. 
- Standard deviation of the residual, standard error of the estimate, MSE...
- We will refer to this as sigma, and use estimated sigma, as we do not know the population value

`$$\hat{\sigma}$$`

- Interpreted in original units (cf R2)
- Standard deviation of Y not accounted by model (ie residuals)

---
# Residual standard error and sigma



```r
summary(fit.1)$sigma 
```

```
## [1] 1.589008
```

```r
psych::describe(galton.data.1$.resid)
```

```
##    vars   n mean   sd median trimmed  mad   min  max range skew kurtosis   se
## X1    1 928    0 1.59  -0.15    0.02 1.52 -4.67 4.27  8.94 -0.1    -0.17 0.05
```

---
## Residual standard error and sigma


```r
summary(fit.1)$sigma 
```

```
## [1] 1.589008
```

```r
psych::describe(galton.data$parent)
```

```
##    vars   n  mean   sd median trimmed  mad min max range  skew kurtosis   se
## X1    1 928 68.31 1.79   68.5   68.32 1.48  64  73     9 -0.04     0.05 0.06
```

- Because the size of `\(\hat{\sigma}\)` depends on both how well the model does as well as the original units of measurement, it is important to compare it to Y to evaluate. 

---
## Why do we care about sigma? 
- Let's simulate to find out
- Data generating process:

$$Y_{i} \sim \mathcal{N}(\mu,\,\sigma)\ $$
- in english: Our DV for individual i is distributed normally with a mean of mu and a standard deviation of sigma  
&lt;br&gt;
- this describes how we think our DVs are generated, and the paramters of interest      
- a standard regression model assumes this, but we will see other DGPs such as binomial or poisson that do not  
- for normal, `\(\mu\)` gets all the focus but `\(\sigma\)` is just as important

---
- Our plan is to fix mu and then vary sigma to see what happens

```r
set.seed(1234)
x.1 &lt;- rnorm(1000, 0, 1)
e.1 &lt;- rnorm(1000, 0, 1)
y.1 &lt;- .5 + .55 * x.1 + e.1
d.1 &lt;- data.frame(x.1,y.1)
m.1 &lt;- lm(y.1 ~ x.1, data = d.1)
```

---
.tiny[

```r
summary(m.1)
```

```
## 
## Call:
## lm(formula = y.1 ~ x.1, data = d.1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1661 -0.6439  0.0145  0.6537  3.0684 
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept)  0.51599    0.03100   16.64 &lt;0.0000000000000002 ***
## x.1          0.60571    0.03109   19.48 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.9801 on 998 degrees of freedom
## Multiple R-squared:  0.2755,	Adjusted R-squared:  0.2748 
## F-statistic: 379.5 on 1 and 998 DF,  p-value: &lt; 0.00000000000000022
```
]
---

&lt;img src="regression-4_files/figure-html/unnamed-chunk-27-1.png" width="504" /&gt;

---
Again, but with a larger sigma

```r
set.seed(987)
e.2 &lt;- rnorm(1000, 0, 2) # larger sigma
y.2 &lt;- .5 + .55 * x.1 + e.2 # same Xs, same mu (.5)
d.2 &lt;- data.frame(x.1,y.2)
m.2 &lt;- lm(y.2 ~ x.1, data = d.2)
```


---
.tiny[

```r
summary(m.2)
```

```
## 
## Call:
## lm(formula = y.2 ~ x.1, data = d.2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.6267 -1.4359 -0.0192  1.4480  6.3439 
## 
## Coefficients:
##             Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)  0.52137    0.06345   8.217 0.000000000000000643 ***
## x.1          0.59823    0.06363   9.402 &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.006 on 998 degrees of freedom
## Multiple R-squared:  0.08136,	Adjusted R-squared:  0.08044 
## F-statistic: 88.39 on 1 and 998 DF,  p-value: &lt; 0.00000000000000022
```
]
---
&lt;img src="regression-4_files/figure-html/unnamed-chunk-30-1.png" width="504" /&gt;

---
# R2 and residual standard deviation
- Two sides of same coin: one in original units, the other standardized 
- R2 can be tricky because the numerator and denominator can be changed in different ways. 

- For example, if variance in Y is changed but with the same regression model and residual standard error, R2 could decline or increase

---
# Standard errors for b
- Represent our uncertainty (noise) in our estimate of the regression coefficient 
- Different from sigma/residual standard error (but proportional to)  
- Sigma is about your entire model whereas se of b is about a single IV  
- (see equation and disucssion about se of bs below)

---
# Inferential tests
- Omnibus test

`$$H_{0}: \rho_{XY}^2= 0$$`
`$$H_{1}: \rho_{XY}^2 \neq 0$$`

`$$F = \frac{MS_{regression}}{MS_{residial}}$$`
---
# Model comparison

- The basic idea is asking how much variance remains unexplained in our model. This "left over" variance can be contrasted with an alternative model/hypothesis. Does adding a new predictor variable help explain more variance or should we stick wtih a parsimonious model?   

- Every model you report do implicitly implies you favoring that over an alternative model, typically the null. This framework allows you to be more flexible and explicit.   

---


```r
fit.1 &lt;- lm(parent ~ child, data = galton.data)
fit.0 &lt;- lm(parent ~ 1, data = galton)
```


---
.small[

```
## 
## Call:
## lm(formula = parent ~ 1, data = galton)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.3082 -0.8082  0.1918  1.1918  4.6918 
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 68.30819    0.05867    1164 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.787 on 927 degrees of freedom
```
]

---
.small[

```
## 
## Call:
## lm(formula = parent ~ child, data = galton.data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.6702 -1.1702 -0.1471  1.1324  4.2722 
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 46.13535    1.41225   32.67 &lt;0.0000000000000002 ***
## child        0.32565    0.02073   15.71 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 1.589 on 926 degrees of freedom
## Multiple R-squared:  0.2105,	Adjusted R-squared:  0.2096 
## F-statistic: 246.8 on 1 and 926 DF,  p-value: &lt; 0.00000000000000022
```
]

---
.tiny[
.pull-left[

```r
anova(fit.0)
```

```
## Analysis of Variance Table
## 
## Response: parent
##            Df Sum Sq Mean Sq F value Pr(&gt;F)
## Residuals 927 2961.4  3.1946
```
]

.pull-right[

```r
anova(fit.1)
```

```
## Analysis of Variance Table
## 
## Response: parent
##            Df  Sum Sq Mean Sq F value                Pr(&gt;F)    
## child       1  623.26  623.26  246.84 &lt; 0.00000000000000022 ***
## Residuals 926 2338.10    2.52                                  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```
]
]
---

```r
anova(fit.1, fit.0)
```

```
## Analysis of Variance Table
## 
## Model 1: parent ~ child
## Model 2: parent ~ 1
##   Res.Df    RSS Df Sum of Sq      F                Pr(&gt;F)    
## 1    926 2338.1                                              
## 2    927 2961.4 -1   -623.26 246.84 &lt; 0.00000000000000022 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---
# Model comparisons 

- Model comparisons are redundent with nil/null hypotheses and coefficient tests right now, but will be more flexible down the road. 
- Key is to start thikning about your implicit alternative models
- The ultimate goal would be to create two models that represent two equally plausible theories
- Theory A is made up of components XYZ, whereas theory B has QRS components. You can then ask which theory(model) is better. 

---
# Regression coefficient

`$$H_{0}: \beta_{1}= 0$$`
`$$H_{1}: \beta_{1} \neq 0$$`

---
# What does the regression coefficient test?
- Does X provide  predictive information? 
- Does X provide any explanatory power regarding the variability of Y? 
- Is the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)
- Is the regression line flat? 
- Are X and Y correlated?  

---
# Regression coefficient
`$$se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}}$$`
`$$t(n-2) = \frac{b_{1}}{se_{b}}$$`
--
- what is standardized equation? 

---
# Intercept

- Same idea, more complex se calculation as the calculation depends on how far the X value (here zero) is away from the mean of X
- Farther from the mean, less information, thus more uncertainty 
- We will come back and see this equation later

---
# Confidence interval for coefficents

- Same equation as we've been working with
- Estimate plus minus 1.96*se

---
# Confidence bands for regression line
&lt;img src="regression-4_files/figure-html/unnamed-chunk-37-1.png" width="504" /&gt;

---
.pull-left[
- Compare mean estimate for height of 70 based on regression vs binning
- Model uses all data where binning uses much less
]

.pull-right[
&lt;img src="regression-4_files/figure-html/unnamed-chunk-38-1.png" width="504" /&gt;
]

---
# Confidence Bands

`$$\hat{Y}\pm t_{critical} * se_{residual}*\sqrt{\frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}$$`


---
# Prediction band
- We are predicting and individual i's score, not the Y hat for a particular level of X. (A new Yi given x, rather than Ybar given x)
- Because there is greater variation in predicting an individual value rather than a collection of individual values (ie the mean) the prediction band is greater
- Combines unknown variability in 1) the estimated mean (as reflected in se of b) 2) peoples scores around  mean (residual standard error) 

`$$\hat{Y}\pm t_{critical} * se_{residual}*\sqrt{1+ \frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}$$`

---


```r
temp_var &lt;- predict(fit.1, interval="prediction")
new_df &lt;- cbind(galton.data, temp_var)
pred &lt;- ggplot(new_df, aes(x=child, y=parent))+
       geom_point() +   
  geom_smooth(method=lm,se=TRUE) +
 geom_ribbon(aes(ymin = lwr, ymax = upr), 
               fill = "blue", alpha = 0.1)
```


---

```r
pred
```

&lt;img src="regression-4_files/figure-html/unnamed-chunk-40-1.png" width="504" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
