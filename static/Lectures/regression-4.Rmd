---
title: "Univariate Regression"
author: Josh Jackson
date: "1-30-20"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    lib_dir: libs
    css: xaringan-themer.css
    seal: false
    nature:
      highlightStyle: github
      highlightLines: true
      ratio: "16:9"
---

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
mono_light(
  base_color = "#23395b",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Droid Mono"),
)
```

<style type="text/css">
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}

.small .remark-code { 
  font-size: 80% !important;
}
.tiny .remark-code {
  font-size: 65% !important;
}
</style>

# Last time/this time

```{r setup,echo=FALSE, message = FALSE}
knitr::opts_chunk$set(fig.retina = 3, warning = FALSE, message = FALSE)

options(scipen=999)

library(psych)
library(ggplot2)
library(broom)
library(dplyr)



```


- Last time, we looked at associations with two continuous variables as indexed via correlations
- This time, we will extend regression framework to incorporate continous predictors.

---
# We want to make eduated guesses
- E(Y|X)
- The regression of Y (DV) on X (IV) corresponds to the line that gives the mean value of Y corresponding to each possible value of X
- "Our best guess" regardless of whether our model includes categories or continuous predictor variables
- We will evaluate our guesses based on how far we are away from the mean...but how do we come up with those guesses in the first place?


---
# Regression Equation

$$Y = b_{0} + b_{1}X +e$$
$$\hat{Y} = b_{0} + b_{1}X$$



---
# OLS
- How do we find the regression estimates and thus our predicted values? 
- Ordinary Least Squares (OLS) estimation 
- Minimizes deviations 

$$min\sum(Y_{i}-\hat{Y})^{2}$$ 

- Other estimation procedures possible (and necessary in some cases)

---

```{r,echo=FALSE, message=FALSE, cache=TRUE}
library(psych)
library(ggplot2)
library(broom)

set.seed(123)
x.1 <- rnorm(10, 0, 1)
e.1 <- rnorm(10, 0, 2)
y.1 <- .5 + .55 * x.1 + e.1
d.1 <- data.frame(x.1,y.1)
m.1 <- lm(y.1 ~ x.1, data = d.1)
d1.f<- augment(m.1)
```

```{r, echo=FALSE, cache=TRUE}

ggplot(d1.f , aes(x=x.1, y=y.1)) +
    geom_point() +
  geom_smooth(method = lm, se = FALSE)


```

---

```{r, echo=FALSE}
ggplot(d1.f , aes(x=x.1, y=y.1)) +
    geom_point() +
  geom_point(aes(y = .fitted), shape = 1) 
```


---

```{r, echo=FALSE}
ggplot(d1.f , aes(x=x.1, y=y.1)) +
    geom_point() +
  geom_point(aes(y = .fitted), shape = 1) +
  geom_segment(aes( xend = x.1, yend = .fitted))
```



---

```{r, echo=FALSE}
ggplot(d1.f , aes(x=x.1, y=y.1)) +
    geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  geom_point(aes(y = .fitted), shape = 1) +
  geom_segment(aes( xend = x.1, yend = .fitted))
```


---
# Regression coefficient
$$b_{1} = \frac{cov_{XY}}{s_{x}^{2}} = r_{xy} \frac{s_{y}}{s_{x}}$$
--

- The regression coefficient (slope) equals the estimated change in Y for a 1-unit change in X  
- For nominal variables using dummy coding (0 and 1s) the regression coefficient equals the difference in means between the two groups (remember, correlation can be used for nominal variables) 

---
# Standardized regression
- Regression using z-scores for Y and X
- Correlation equals standardized regression coefficent
$$b_{1} = r_{xy} \frac{s_{y}}{s_{x}}$$
$$r_{xy} = b_{1} \frac{s_{x}}{s_{y}}$$
$$\beta_{1} = b_{1}^{*} = r_{xy}$$  

---
# Standardized regression equation
$$Y = b_{1}^{*}X + e$$
- What is missing? 
- What does this equation tell you about the function of the intercept? 

---
#  Raw score regression equation
- Re-write equation to include means of Y and X 
- Intercept serves to adjust for differences in means between X and Y

$$\hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X})$$
- If standardized, intercept drops out, otherwise intercept is where regression line crosses the y-axis at X = 0  
- Notice that when X = $\bar{X}$ the regression line goes through  $\bar{Y}$. This is true for all regressions such that the regression line must pass through $\bar{X}$ and $\bar{Y}$


---
## Example
.tiny[
```{r, echo=FALSE}
library(psychTools)
galton.data <- galton
fit.1 <- lm(parent ~ child, data = galton.data)
summary(fit.1)
```
]

---

```{r, echo=FALSE, cache=TRUE}
ggplot(galton.data, aes(x=child, y=parent)) +
    geom_point() +    
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE)
```

---
# ANOVA table
```{r}
anova(fit.1)
```


---
# Exploring the lm object

```{r, warning=FALSE}
library(broom)
galton.data.1 <- augment(fit.1, galton.data)
head(galton.data.1)
```


---
# Y-hats vs Ys
```{r}
psych::describe(galton.data.1$.fitted)      
```

```{r}
psych::describe(galton.data.1$parent)
```

---

```{r,echo=FALSE, cache=TRUE}
library(gridExtra)

gg1 <- galton.data.1 %>% 
  ggplot +
  geom_density(aes(x = .fitted)) +
  xlim(64,74)
gg2 <- galton.data.1 %>% 
  ggplot +
  geom_density(aes(x = parent)) +
  xlim(64,74)

gridExtra::grid.arrange(gg1,gg2)

```


---
# Residuals vs Ys

.pull-left[
$$\epsilon \sim N(0,\sigma)$$ 
- variation that is left over in Y, after accounting for X
]

.pull-right[
```{r}
psych::describe(galton.data.1$.resid)
psych::describe(galton.data.1$parent)
```
]

---
```{r, echo=FALSE,}

gg3 <- galton.data.1 %>% 
  ggplot +
  geom_density(aes(x = .resid)) +
  xlim(-5,5)
gg4 <- galton.data.1 %>% 
  ggplot +
  geom_density(aes(x = parent)) +
  xlim(64,74)

gridExtra::grid.arrange(gg3,gg4)
```


---
# Residuals

- Dispersion of residuals can be thought of as what is left over in Y that is not explained by our model. As residuals get smaller on average so will the SD of the residuals. 
- Sigma (the SD of residuals) can be thought of how much left over in Y that we cannot explain by our model

---
# Residuals are not assocaited with X

```{r, echo = FALSE, cache=TRUE,  fig.show='hide'}
ggplot(galton.data.1, aes(x=.resid, y=child)) +
    geom_point() +    
    geom_smooth(method=lm)
```

---
## and yhat
.pull-left[
```{r, cache=TRUE, fig.show='hide'}
ggplot(galton.data.1, aes(x=.resid, y=.fitted)) +
    geom_point() +    
    geom_smooth(method=lm)
```
]
.pull-right[
```{r, cache=TRUE, echo = FALSE}
ggplot(galton.data.1, aes(x=.resid, y=.fitted)) +
    geom_point() +    
    geom_smooth(method=lm)
```
]

---
# r(fitted and X) = 1
```{r, cache=TRUE, echo = FALSE}
ggplot(galton.data.1, aes(x=.fitted, y=child)) +
    geom_point() +    
    geom_smooth(method=lm)
```

---
- Residuals are not correlated with X and $\hat{Y}$ because those two are perfectly correlated with one another.
- X and $\hat{Y}$ represent the *same* information. We use our model (X) to make a prediction ( $\hat{Y}$ ). The predictions are entirely based on the model. 
- There is no correlation between residuals with X and $\hat{Y}$ because they are created by subtracting them out of Y ( $\varepsilon$  = Y- $\hat{Y}$)
- Sigma (the SD of residuals) can be thought of how much left over in Y after we take out all the information our model provides

---
# Statistical Inference
- The way the world is = our model + error
- How good is our model? Does it "fit" the data well? 
 
- Consider the case with no correlation btw X and Y
$$\hat{Y} = \bar{Y} + r_{xy} \frac{s_{y}}{s_{x}}(X-\bar{X})$$
$$\hat{Y} = \bar{Y}$$

---
# Partitioning variance in Y
- To the extent that we can generate different predicted values of Y based on the values of the predictors, we are doing well in our prediction

$$\sum (Y - \bar{Y})^2 = \sum (\hat{Y} -\bar{Y})^2 + \sum(Y - \hat{Y})^2$$

- SS total = SS regression + SS residual (or error)

$$\frac{s_{regression}^2}{s_{y}^2} = \frac{SS_{regression}}{SS_{Y}} = R^2$$


---
# Example
```{r}

summary(fit.1)$r.squared
```

---
```{r}
cor.test(galton.data$parent, galton.data$child)
```

---
## Calculating R2

```{r, echo=FALSE}

anova(fit.1)

```

$$\frac{SS_{regression}}{SS_{Y}} = R^2$$

$${SS_{residual}} = (1- R^2){SS_{Y}}$$
---
# Mean square error (MSE)
- AKA means square residual/within
- AKA square of residual standard error/deviation (sigma)
- Unbiased estimate of error variance
- Measure of discrepancy between the data and the model
- The MSE is the variance of data around the fitted regression line
- Just like MSwithin was variance around predicted group means

---
## Residual standard error
MSE = 2.52 
.tiny[
```{r, echo=FALSE}
summary(fit.1)
```
]

---
# Residual standard error and sigma
- So many names to represent the spread of data around the regression line. 
- Standard deviation of the residual, standard error of the estimate, MSE...
- We will refer to this as sigma, and use estimated sigma, as we do not know the population value

$$\hat{\sigma}$$

- Interpreted in original units (cf R2)
- Standard deviation of Y not accounted by model (ie residuals)

---
# Residual standard error and sigma


```{r}
summary(fit.1)$sigma 
psych::describe(galton.data.1$.resid)
```

---
## Residual standard error and sigma

```{r}
summary(fit.1)$sigma 
psych::describe(galton.data$parent)
```

- Because the size of $\hat{\sigma}$ depends on both how well the model does as well as the original units of measurement, it is important to compare it to Y to evaluate. 

---
## Why do we care about sigma? 
- Let's simulate to find out
- Data generating process:

$$Y_{i} \sim \mathcal{N}(\mu,\,\sigma)\ $$
- in english: Our DV for individual i is distributed normally with a mean of mu and a standard deviation of sigma  
<br>
- this describes how we think our DVs are generated, and the paramters of interest      
- a standard regression model assumes this, but we will see other DGPs such as binomial or poisson that do not  
- for normal, $\mu$ gets all the focus but $\sigma$ is just as important

---
- Our plan is to fix mu and then vary sigma to see what happens
```{r}
set.seed(1234)
x.1 <- rnorm(1000, 0, 1)
e.1 <- rnorm(1000, 0, 1)
y.1 <- .5 + .55 * x.1 + e.1
d.1 <- data.frame(x.1,y.1)
m.1 <- lm(y.1 ~ x.1, data = d.1)
```

---
.tiny[
```{r}
summary(m.1)

```
]
---

```{r, echo=FALSE, warning=FALSE}
library(ggplot2)
ggplot(d.1, aes(x = x.1,y =  y.1)) +
    geom_point() +    
    geom_smooth(method = lm) +
   scale_x_continuous(limits = c(-3, 3))  +
  scale_y_continuous(limits = c(-6, 6))

```

---
Again, but with a larger sigma
```{r}
set.seed(987)
e.2 <- rnorm(1000, 0, 2) # larger sigma
y.2 <- .5 + .55 * x.1 + e.2 # same Xs, same mu (.5)
d.2 <- data.frame(x.1,y.2)
m.2 <- lm(y.2 ~ x.1, data = d.2)
```


---
.tiny[
```{r}
summary(m.2)

```
]
---
```{r,echo=FALSE, warning=FALSE}
ggplot(d.2, aes(x=x.1, y=y.2)) +
    geom_point() +    
    geom_smooth(method=lm) +
  scale_x_continuous(limits = c(-3, 3))  +
  scale_y_continuous(limits = c(-6, 6))
```

---
# R2 and residual standard deviation
- Two sides of same coin: one in original units, the other standardized 
- R2 can be tricky because the numerator and denominator can be changed in different ways. 

- For example, if variance in Y is changed but with the same regression model and residual standard error, R2 could decline or increase

---
# Standard errors for b
- Represent our uncertainty (noise) in our estimate of the regression coefficient 
- Different from sigma/residual standard error (but proportional to)  
- Sigma is about your entire model whereas se of b is about a single IV  
- (see equation and disucssion about se of bs below)

---
# Inferential tests
- Omnibus test

$$H_{0}: \rho_{XY}^2= 0$$
$$H_{1}: \rho_{XY}^2 \neq 0$$

$$F = \frac{MS_{regression}}{MS_{residial}}$$
---
# Model comparison

- The basic idea is asking how much variance remains unexplained in our model. This "left over" variance can be contrasted with an alternative model/hypothesis. Does adding a new predictor variable help explain more variance or should we stick wtih a parsimonious model?   

- Every model you report do implicitly implies you favoring that over an alternative model, typically the null. This framework allows you to be more flexible and explicit.   

---

```{r}
fit.1 <- lm(parent ~ child, data = galton.data)
fit.0 <- lm(parent ~ 1, data = galton)
```


---
.small[
```{r, echo=FALSE}
summary(fit.0)
```
]

---
.small[
```{r, echo=FALSE}
summary(fit.1)
```
]

---
.tiny[
.pull-left[
```{r}
anova(fit.0)
```
]

.pull-right[
```{r}
anova(fit.1)
```
]
]
---
```{r}
anova(fit.1, fit.0)
```

---
# Model comparisons 

- Model comparisons are redundent with nil/null hypotheses and coefficient tests right now, but will be more flexible down the road. 
- Key is to start thikning about your implicit alternative models
- The ultimate goal would be to create two models that represent two equally plausible theories
- Theory A is made up of components XYZ, whereas theory B has QRS components. You can then ask which theory(model) is better. 

---
# Regression coefficient

$$H_{0}: \beta_{1}= 0$$
$$H_{1}: \beta_{1} \neq 0$$

---
# What does the regression coefficient test?
- Does X provide  predictive information? 
- Does X provide any explanatory power regarding the variability of Y? 
- Is the the average value the best guess (i.e., is Y bar equal to the predicted value of Y?)
- Is the regression line flat? 
- Are X and Y correlated?  

---
# Regression coefficient
$$se_{b} = \frac{s_{Y}}{s_{X}}{\sqrt{\frac {1-r_{xy}^2}{n-2}}}$$
$$t(n-2) = \frac{b_{1}}{se_{b}}$$
--
- what is standardized equation? 

---
# Intercept

- Same idea, more complex se calculation as the calculation depends on how far the X value (here zero) is away from the mean of X
- Farther from the mean, less information, thus more uncertainty 
- We will come back and see this equation later

---
# Confidence interval for coefficents

- Same equation as we've been working with
- Estimate plus minus 1.96*se

---
# Confidence bands for regression line
```{r, echo=FALSE, message=FALSE}
set.seed(123)

px.1 <- rnorm(1000, 0, 1)
pe.1 <- rnorm(1000, 0, 1)
py.1 <- .5 + .55 * px.1 + pe.1
pd.1 <- data.frame(px.1,py.1)

px.2 <- rnorm(100, 0, 1)
pe.2 <- rnorm(100, 0, 1)
py.2 <- .5 + .55 * px.2 + pe.2
pd.2 <- data.frame(px.2,py.2)


p1 <- ggplot(pd.1, aes(x = px.1,y =  py.1)) +
    geom_point() +    
    geom_smooth(method = lm) +
   scale_x_continuous(limits = c(-3, 3))  +
  scale_y_continuous(limits = c(-3, 3))

p2 <- ggplot(pd.2, aes(x=px.2, y=py.2)) +
    geom_point() +    
    geom_smooth(method=lm) +
  scale_x_continuous(limits = c(-3, 3))  +
  scale_y_continuous(limits = c(-3, 3))

library(cowplot)
plot_grid(p1, p2, ncol=2, labels = c("N = 1000", "N = 100"))

```

---
.pull-left[
- Compare mean estimate for height of 70 based on regression vs binning
- Model uses all data where binning uses much less
]

.pull-right[
```{r, echo=FALSE}
ggplot(galton.data, aes(x=child, y=parent)) +
      geom_point() +   
  geom_smooth(method=lm,   # Add linear regression line
                se=TRUE) 
```
]

---
# Confidence Bands

$$\hat{Y}\pm t_{critical} * se_{residual}*\sqrt{\frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}$$


---
# Prediction band
- We are predicting and individual i's score, not the Y hat for a particular level of X. (A new Yi given x, rather than Ybar given x)
- Because there is greater variation in predicting an individual value rather than a collection of individual values (ie the mean) the prediction band is greater
- Combines unknown variability in 1) the estimated mean (as reflected in se of b) 2) peoples scores around  mean (residual standard error) 

$$\hat{Y}\pm t_{critical} * se_{residual}*\sqrt{1+ \frac {1}{n}+\frac{(X-\bar{X})^2}{(n-1)s_{X}^2}}$$

---

```{r, warning=FALSE}
temp_var <- predict(fit.1, interval="prediction")
new_df <- cbind(galton.data, temp_var)
pred <- ggplot(new_df, aes(x=child, y=parent))+
       geom_point() +   
  geom_smooth(method=lm,se=TRUE) +
 geom_ribbon(aes(ymin = lwr, ymax = upr), 
               fill = "blue", alpha = 0.1)
```


---
```{r}
pred
```








